{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Feels\n",
    "\n",
    "## Topical sentiment analysis from recent twitter posts\n",
    "\n",
    "This notebook:\n",
    "\n",
    "* Builds (or reuses) a model built from positive and negative tweets\n",
    "* Fetches the recent history of posted tweets by one or more keywords\n",
    "* Analyzes the retrieved tweets for the overall sentiment (positive or negative) for those keywords and reports it\n",
    "* Takes a new potential tweet and predicts its sentiment\n",
    "\n",
    "### An example of using Naive Bayes classification.\n",
    "\n",
    "Let's train an Naive Bayes Classifier how to \"read\" tweets, derive a topic, and guess  whether the tweet was positive or negative toward the topic.\n",
    "\n",
    "We start by importing the libraries we need, as well as REcreating the source text file of tweets. This takes a long time, so if you skip it, the build will use the most recent file of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary methods from tweepy library\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to update the tweet data file named 'fetched_tweets.txt' OR SKIP IT, as it runs a LONG time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "#\n",
    "#    REcreating tweets file; skip this cell or be prepared to wait!\n",
    "#\n",
    "############################################################################\n",
    "now = datetime.now() # current date and time\n",
    "\n",
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"29859964-YADj45SokICNoDlpNVyo4oW36fj526S3hQeDMpoXY\"\n",
    "access_token_secret = \"CyFZ2wEo3CWA432IAtsFbk9EwSC3Hrr9PKkfZdXPeOHCQ\"\n",
    "consumer_key = \"HIMCEsg8p7sQedSbOTrrjjH6W\"\n",
    "consumer_secret = \"lsIvwa04RsNB8KhJ1yBRa1LqLt0oNM2ffefYtKixx1Zn55L2gw\"\n",
    "\n",
    "#This is a basic listener that just prints received tweets to stdout.\n",
    "class StdOutListener(StreamListener):\n",
    "    \n",
    "    def __init__(self, api=None):\n",
    "        super(StdOutListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        \n",
    "    def on_status(self, data):\n",
    "        # print(data)\n",
    "        startDate = datetime.today()\n",
    "        if self.num_tweets < 10000:\n",
    "            if data.created_at > startDate:\n",
    "                with open('fetched_tweets.txt','a') as tf:\n",
    "                    if hasattr(data, \"retweeted_status\"):  # Check if Retweet\n",
    "                        try:\n",
    "                            thisStatus = data.retweeted_status.extended_tweet[\"full_text\"]\n",
    "                        except AttributeError:\n",
    "                            thisStatus = data.retweeted_status.text\n",
    "                    else:\n",
    "                        try:\n",
    "                            thisStatus = data.extended_tweet[\"full_text\"]\n",
    "                        except AttributeError:\n",
    "                            thisStatus = data.text\n",
    "                    s = \"\\t\"\n",
    "                    timestampStr = data.created_at.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n",
    "                    tf.write(thisStatus)\n",
    "                    tf.write(\"\\n\")\n",
    "            self.num_tweets += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #This handles Twitter authentication and the connection to Twitter Streaming API\n",
    "    l = StdOutListener()\n",
    "    auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    stream = Stream(auth, l, tweet_mode='extended', include_rts=False)\n",
    "\n",
    "    ##############################\n",
    "    #\n",
    "    # UPDATE THE KEYWORDS BELOW!\n",
    "    #\n",
    "    ##############################\n",
    "    \n",
    "    keywords = ['Trump','corona','virus']\n",
    "    f = open(\"fetched_tweets.txt\", \"w\")\n",
    "    s = \"|||\"\n",
    "    keyargs = keywords + [now.strftime(\"%Y-%m-%d\")]\n",
    "    f.write(s.join(keyargs))\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    #This line filter Twitter Streams to capture data by the keywords: 'python', 'javascript', 'ruby'\n",
    "    stream.filter(languages=[\"en\"],track=keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, reuse the existing model OR import our training data, then retrain and test the model.\n",
    "The dataset includes 5,000 positive tweets and 5,000 negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/toddrimes/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/toddrimes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/toddrimes/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/toddrimes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/toddrimes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.274706867671692\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "import pickle\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    global useExistingModel\n",
    "    useExistingModel = True\n",
    "\n",
    "    if useExistingModel == False:\n",
    "\n",
    "        positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "        negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "        #text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "        #tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "\n",
    "        positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "        negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "        positive_cleaned_tokens_list = []\n",
    "        negative_cleaned_tokens_list = []\n",
    "\n",
    "        for tokens in positive_tweet_tokens:\n",
    "            positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "        for tokens in negative_tweet_tokens:\n",
    "            negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "        all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "        freq_dist_pos = FreqDist(all_pos_words)\n",
    "        print(freq_dist_pos.most_common(10))\n",
    "\n",
    "        positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "        negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "        positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                             for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "        negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                             for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "        dataset = positive_dataset + negative_dataset\n",
    "\n",
    "        random.shuffle(dataset)\n",
    "\n",
    "        train_data = dataset[:8000]\n",
    "        test_data = dataset[2000:]\n",
    "\n",
    "        classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "        print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "        print(classifier.show_most_informative_features(10))\n",
    "        \n",
    "        f = open('tweet_classifier.pickle','wb')\n",
    "        pickle.dump(classifier, f)\n",
    "        f.close()\n",
    "\n",
    "        # custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "        # custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "        # print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))\n",
    "\n",
    "    elif useExistingModel:\n",
    "        with open('tweet_classifier.pickle','rb') as f:\n",
    "            classifier = pickle.load(f)\n",
    "            f.close()\n",
    "    \n",
    "    counter = 0\n",
    "    keyargs = \"\"\n",
    "    date = \"\"\n",
    "    \n",
    "    my_dict = ({'Negative':1,'Positive':1})\n",
    "    with open('fetched_tweets.txt','r') as tf:\n",
    "        for x in tf:\n",
    "            if counter == 0:\n",
    "                keyargs = x.split(\"|||\")\n",
    "                date = keyargs.pop()\n",
    "            custom_tweet = x\n",
    "            custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "            sentiment = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "            #if counter < 30:\n",
    "            #    print (\"first sentiment is \", sentiment)\n",
    "            counter += 1\n",
    "            my_dict[sentiment] +=1\n",
    "            \n",
    "    sentimentRatio = 0\n",
    "    if my_dict['Positive'] > my_dict['Negative']:\n",
    "        sentimentRatio = my_dict['Positive'] / my_dict['Negative']\n",
    "    elif my_dict['Negative'] > my_dict['Positive']:\n",
    "        sentimentRatio = -1 * (my_dict['Negative'] / my_dict['Positive'])\n",
    "    \n",
    "    print(sentimentRatio)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a feel for what this data looks like. Let's look at the first training feature, which should represent a written movie review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# <span style=\"color:green\">POSITIVE</span> sentiment\n",
       "    Overall, the tweets about ['Trump', 'corona', 'virus'] are postive at a ratio of 2:1\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "NEGATIVE = sentimentRatio < 0\n",
    "POSITIVE = sentimentRatio > 0\n",
    "\n",
    "rSentimentRatio = round(sentimentRatio)\n",
    "\n",
    "if NEGATIVE:\n",
    "    display(Markdown(\"\"\"# <span style=\"color:red\">NEGATIVE</span> sentiment\n",
    "    Overall, the tweets about {} are negative at a ratio of {}:1\n",
    "    \"\"\".format(keywords, rSentimentRatio)))\n",
    "if POSITIVE:\n",
    "    display(Markdown(\"\"\"# <span style=\"color:green\">POSITIVE</span> sentiment\n",
    "    Overall, the tweets about {} are postive at a ratio of {}:1\n",
    "    \"\"\".format(keywords, rSentimentRatio)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
